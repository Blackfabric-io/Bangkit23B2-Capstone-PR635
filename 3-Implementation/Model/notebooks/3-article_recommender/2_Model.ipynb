{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e45e060",
   "metadata": {},
   "source": [
    "# Proof-of-Concept: Simple Article Recommendation - Model Development\n",
    "\n",
    "This notebook demonstrates a basic prototype implementation of a content recommendation system for the NutriGenius proof-of-concept.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup](#setup)\n",
    "3. [Loading Processed Data](#loading-data)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Development](#model-development)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Personalization Features](#personalization)\n",
    "8. [Model Export](#export)\n",
    "9. [Conclusion](#conclusion)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook implements a simplified article recommendation prototype for the NutriGenius proof-of-concept. We intentionally use basic approaches that:\n",
    "\n",
    "1. Require minimal data preparation (works with our synthetic dataset)\n",
    "2. Implement classical TF-IDF and LSA rather than complex neural approaches\n",
    "3. Provide simple but effective content-based recommendations\n",
    "4. Can be easily integrated into a prototype mobile application\n",
    "\n",
    "The goal is to demonstrate that even with these simplifications, we can deliver reasonable article recommendations based on user characteristics and interests. This approach is well-suited for a proof-of-concept while requiring minimal development time and resources.\n",
    "\n",
    "> **Note**: This prototype intentionally uses simplified algorithms instead of state-of-the-art recommendation techniques to focus on demonstrating the concept with minimal complexity.\n",
    "\n",
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd53222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Add project root to path\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '../..')))\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils.common import load_config, create_directory\n",
    "from src.utils.data_processing import clean_text, tokenize_text\n",
    "from src.article_recommender import build_recommendation_model, get_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b103a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "CONFIG_PATH = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '../../config/model_config.yaml'))\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Extract relevant configuration\n",
    "article_config = config['article_recommender']\n",
    "dataset_config = config['dataset']['articles']\n",
    "model_path_config = config['model_paths']['article_recommender']\n",
    "\n",
    "# Define paths from config\n",
    "ARTICLES_DATA_FILE = dataset_config['data_file']\n",
    "ARTICLES_PROCESSED_FILE = dataset_config['processed_file']\n",
    "RECOMMENDER_MODEL_PATH = model_path_config['model']\n",
    "\n",
    "# Create necessary directories\n",
    "create_directory(os.path.dirname(ARTICLES_DATA_FILE))\n",
    "create_directory(os.path.dirname(ARTICLES_PROCESSED_FILE))\n",
    "create_directory(os.path.dirname(RECOMMENDER_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d63abd",
   "metadata": {},
   "source": [
    "## 3. Loading Processed Data\n",
    "\n",
    "First, let's load the processed article data from the EDA notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed data exists\n",
    "if os.path.exists(ARTICLES_PROCESSED_FILE):\n",
    "    print(f\"Loading processed articles from {ARTICLES_PROCESSED_FILE}\")\n",
    "    articles_df = pd.read_csv(ARTICLES_PROCESSED_FILE)\n",
    "else:\n",
    "    # If processed data doesn't exist, load raw data\n",
    "    if os.path.exists(ARTICLES_DATA_FILE):\n",
    "        print(f\"Loading raw articles from {ARTICLES_DATA_FILE}\")\n",
    "        articles_df = pd.read_csv(ARTICLES_DATA_FILE)\n",
    "        \n",
    "        # Process the data (similar to EDA notebook)\n",
    "        # ... preprocessing code would go here\n",
    "    else:\n",
    "        # If neither exists, create sample dataset\n",
    "        from src.article_recommender import create_sample_article_dataset\n",
    "        print(\"Creating sample article dataset\")\n",
    "        create_sample_article_dataset(ARTICLES_DATA_FILE, num_articles=200)\n",
    "        articles_df = pd.read_csv(ARTICLES_DATA_FILE)\n",
    "        \n",
    "        # Process the data (similar to EDA notebook)\n",
    "        # ... preprocessing code would go here\n",
    "\n",
    "print(f\"Loaded dataset with {len(articles_df)} articles\")\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the processed text columns already\n",
    "if 'processed_content' not in articles_df.columns:\n",
    "    print(\"Processing text data...\")\n",
    "    # Define text preprocessing function\n",
    "    def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        if lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Rejoin into string\n",
    "        processed_text = ' '.join(tokens)\n",
    "        \n",
    "        return processed_text\n",
    "    \n",
    "    # Apply preprocessing to article contents\n",
    "    articles_df['processed_content'] = articles_df['content'].apply(\n",
    "        lambda x: preprocess_text(x, remove_stopwords=True, lemmatize=True)\n",
    "    )\n",
    "    \n",
    "    # Process article titles\n",
    "    articles_df['processed_title'] = articles_df['title'].apply(\n",
    "        lambda x: preprocess_text(x, remove_stopwords=True, lemmatize=True)\n",
    "    )\n",
    "    \n",
    "    # Process article tags\n",
    "    articles_df['processed_tags'] = articles_df['tags'].apply(\n",
    "        lambda x: preprocess_text(x.replace(',', ' '), remove_stopwords=False, lemmatize=True)\n",
    "    )\n",
    "    \n",
    "    # Combine all processed text fields\n",
    "    articles_df['combined_text'] = (\n",
    "        articles_df['processed_title'] + ' ' + \n",
    "        articles_df['processed_content'] + ' ' + \n",
    "        articles_df['processed_tags'] + ' ' + \n",
    "        articles_df['category'].apply(lambda x: x.lower().replace(' ', '_'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4620f",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Let's create numerical representations of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer with parameters from config\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=article_config['max_features'],\n",
    "    lowercase=article_config['preprocessing']['lowercase'],\n",
    "    stop_words='english' if article_config['preprocessing']['remove_stopwords'] else None\n",
    ")\n",
    "\n",
    "# Fit and transform the combined text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_df['combined_text'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction using LSA (Latent Semantic Analysis)\n",
    "n_components = min(100, tfidf_matrix.shape[1] - 1)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"LSA matrix shape: {lsa_matrix.shape}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_components + 1), svd.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by LSA Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fdad0",
   "metadata": {},
   "source": [
    "## 5. Model Development\n",
    "\n",
    "Now, let's develop our recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5374e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix using LSA features\n",
    "lsa_similarity = cosine_similarity(lsa_matrix)\n",
    "print(f\"LSA similarity matrix shape: {lsa_similarity.shape}\")\n",
    "\n",
    "# Also keep the TF-IDF similarity for comparison\n",
    "tfidf_similarity = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03790d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get article recommendations based on article index\n",
    "def get_recommendations_by_index(article_index, similarity_matrix, df, top_n=5):\n",
    "    # Get similarity scores for the article\n",
    "    sim_scores = list(enumerate(similarity_matrix[article_index]))\n",
    "    \n",
    "    # Remove the article itself\n",
    "    sim_scores = [score for score in sim_scores if score[0] != article_index]\n",
    "    \n",
    "    # Sort based on similarity score\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top N similar articles\n",
    "    sim_scores = sim_scores[:top_n]\n",
    "    \n",
    "    # Get article indices and scores\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "    similarity_scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # Create a DataFrame with recommended articles\n",
    "    recommendations = df.iloc[article_indices][['article_id', 'title', 'category', 'tags']].copy()\n",
    "    recommendations['similarity'] = similarity_scores\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc684d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get query-based recommendations\n",
    "def get_query_recommendations(query, vectorizer, matrix, similarity_type=\"tfidf\", df=articles_df, top_n=5):\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_text(query, remove_stopwords=True, lemmatize=True)\n",
    "    \n",
    "    # Transform the query using the same vectorizer\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    \n",
    "    # Apply SVD if using LSA\n",
    "    if similarity_type == \"lsa\":\n",
    "        query_vector = svd.transform(query_vector)\n",
    "        sim_matrix = lsa_matrix\n",
    "    else:\n",
    "        sim_matrix = matrix\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    if similarity_type == \"lsa\":\n",
    "        cosine_similarities = cosine_similarity(query_vector, sim_matrix).flatten()\n",
    "    else:\n",
    "        cosine_similarities = cosine_similarity(query_vector, matrix).flatten()\n",
    "    \n",
    "    # Get top N similar articles\n",
    "    sim_scores = list(enumerate(cosine_similarities))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    # Get article indices and scores\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "    similarity_scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # Create a DataFrame with recommended articles\n",
    "    recommendations = df.iloc[article_indices][['article_id', 'title', 'category', 'tags']].copy()\n",
    "    recommendations['similarity'] = similarity_scores\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced function for personalized recommendations\n",
    "def get_personalized_recommendations(user_profile, food_items=None, health_status=None, \n",
    "                                     similarity_type=\"lsa\", top_n=5):\n",
    "    # Build query from user profile and interests\n",
    "    query_parts = []\n",
    "    \n",
    "    # Add age-related terms\n",
    "    if 'age' in user_profile:\n",
    "        age = user_profile['age']\n",
    "        if age < 18:\n",
    "            query_parts.append(\"nutrition for children teenagers youth\")\n",
    "        elif age < 30:\n",
    "            query_parts.append(\"nutrition for young adults\")\n",
    "        elif age < 50:\n",
    "            query_parts.append(\"nutrition for adults middle age\")\n",
    "        else:\n",
    "            query_parts.append(\"nutrition for seniors elderly older adults\")\n",
    "    \n",
    "    # Add gender-related terms\n",
    "    if 'gender' in user_profile:\n",
    "        gender = user_profile['gender'].lower()\n",
    "        if gender == 'male':\n",
    "            query_parts.append(\"men's nutrition male health\")\n",
    "        elif gender == 'female':\n",
    "            query_parts.append(\"women's nutrition female health\")\n",
    "    \n",
    "    # Add food items\n",
    "    if food_items:\n",
    "        food_query = \" \".join(food_items)\n",
    "        query_parts.append(f\"nutrition {food_query} recipes diet\")\n",
    "    \n",
    "    # Add health status terms\n",
    "    if health_status:\n",
    "        # BMI-related recommendations\n",
    "        if 'bmi' in health_status:\n",
    "            bmi = health_status['bmi']\n",
    "            if bmi < 18.5:\n",
    "                query_parts.append(\"underweight nutrition gain weight healthy calories protein\")\n",
    "            elif bmi < 25:\n",
    "                query_parts.append(\"healthy weight maintenance balanced diet\")\n",
    "            elif bmi < 30:\n",
    "                query_parts.append(\"overweight nutrition weight management reducing calories\")\n",
    "            else:\n",
    "                query_parts.append(\"obesity weight loss diet plan calorie deficit\")\n",
    "        \n",
    "        # Add other health conditions\n",
    "        if 'conditions' in health_status:\n",
    "            conditions = \" \".join(health_status['conditions'])\n",
    "            query_parts.append(f\"nutrition for {conditions} diet health\")\n",
    "    \n",
    "    # Combine all query parts\n",
    "    combined_query = \" \".join(query_parts)\n",
    "    \n",
    "    # Get recommendations based on the combined query\n",
    "    if similarity_type == \"lsa\":\n",
    "        return get_query_recommendations(combined_query, tfidf_vectorizer, tfidf_matrix, \"lsa\", top_n=top_n), combined_query\n",
    "    else:\n",
    "        return get_query_recommendations(combined_query, tfidf_vectorizer, tfidf_matrix, \"tfidf\", top_n=top_n), combined_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60618d",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF-IDF vs LSA recommendations for a random article\n",
    "random_article_idx = np.random.randint(0, len(articles_df))\n",
    "article = articles_df.iloc[random_article_idx]\n",
    "\n",
    "print(f\"Selected article (ID: {article['article_id']}):\\n{article['title']}\")\n",
    "print(f\"Category: {article['category']}\")\n",
    "print(f\"Tags: {article['tags']}\")\n",
    "print()\n",
    "\n",
    "# Get TF-IDF recommendations\n",
    "tfidf_recommendations = get_recommendations_by_index(random_article_idx, tfidf_similarity, articles_df)\n",
    "\n",
    "print(\"TF-IDF Recommendations:\")\n",
    "for i, (_, row) in enumerate(tfidf_recommendations.iterrows()):\n",
    "    print(f\"{i+1}. {row['title']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"   Category: {row['category']}\")\n",
    "    print(f\"   Tags: {row['tags']}\")\n",
    "    print()\n",
    "\n",
    "# Get LSA recommendations\n",
    "lsa_recommendations = get_recommendations_by_index(random_article_idx, lsa_similarity, articles_df)\n",
    "\n",
    "print(\"LSA Recommendations:\")\n",
    "for i, (_, row) in enumerate(lsa_recommendations.iterrows()):\n",
    "    print(f\"{i+1}. {row['title']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"   Category: {row['category']}\")\n",
    "    print(f\"   Tags: {row['tags']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate query-based recommendations\n",
    "test_queries = [\n",
    "    \"protein rich foods for muscle building\",\n",
    "    \"low calorie foods for weight loss\",\n",
    "    \"nutrition for pregnant women\",\n",
    "    \"managing diabetes through diet\",\n",
    "    \"heart healthy diet\"\n",
    "]\n",
    "\n",
    "# Compare TF-IDF vs LSA for query-based recommendations\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get TF-IDF recommendations\n",
    "    tfidf_query_recs = get_query_recommendations(query, tfidf_vectorizer, tfidf_matrix, \"tfidf\", top_n=3)\n",
    "    \n",
    "    print(\"TF-IDF Recommendations:\")\n",
    "    for i, (_, row) in enumerate(tfidf_query_recs.iterrows()):\n",
    "        print(f\"{i+1}. {row['title']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"   Category: {row['category']}\")\n",
    "    \n",
    "    # Get LSA recommendations\n",
    "    lsa_query_recs = get_query_recommendations(query, tfidf_vectorizer, tfidf_matrix, \"lsa\", top_n=3)\n",
    "    \n",
    "    print(\"\\nLSA Recommendations:\")\n",
    "    for i, (_, row) in enumerate(lsa_query_recs.iterrows()):\n",
    "        print(f\"{i+1}. {row['title']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"   Category: {row['category']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21d817",
   "metadata": {},
   "source": [
    "## 7. Personalization Features\n",
    "\n",
    "Let's test the personalized recommendation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9824a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test personalized recommendations with different user profiles\n",
    "test_profiles = [\n",
    "    {\n",
    "        \"name\": \"Teen Athlete\",\n",
    "        \"profile\": {\"age\": 16, \"gender\": \"male\"},\n",
    "        \"food_items\": [\"chicken\", \"rice\", \"broccoli\", \"protein shake\"],\n",
    "        \"health_status\": {\"bmi\": 21.5, \"conditions\": [\"athletic performance\"]}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Young Adult Vegetarian\",\n",
    "        \"profile\": {\"age\": 25, \"gender\": \"female\"},\n",
    "        \"food_items\": [\"tofu\", \"beans\", \"spinach\", \"quinoa\"],\n",
    "        \"health_status\": {\"bmi\": 19.8, \"conditions\": [\"vegetarian\", \"iron deficiency\"]}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Middle-aged with Diabetes\",\n",
    "        \"profile\": {\"age\": 48, \"gender\": \"male\"},\n",
    "        \"food_items\": [\"chicken breast\", \"brown rice\", \"salad\"],\n",
    "        \"health_status\": {\"bmi\": 29.2, \"conditions\": [\"type 2 diabetes\", \"high blood pressure\"]}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Senior with Heart Condition\",\n",
    "        \"profile\": {\"age\": 72, \"gender\": \"female\"},\n",
    "        \"food_items\": [\"fish\", \"oatmeal\", \"blueberries\", \"walnuts\"],\n",
    "        \"health_status\": {\"bmi\": 26.5, \"conditions\": [\"heart disease\", \"arthritis\"]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test LSA personalized recommendations for each profile\n",
    "for test in test_profiles:\n",
    "    print(f\"User: {test['name']}\")\n",
    "    print(f\"Profile: {test['profile']}\")\n",
    "    print(f\"Food Items: {test['food_items']}\")\n",
    "    print(f\"Health Status: {test['health_status']}\")\n",
    "    print()\n",
    "    \n",
    "    # Get personalized recommendations\n",
    "    recommendations, query = get_personalized_recommendations(\n",
    "        test['profile'], test['food_items'], test['health_status'], \n",
    "        similarity_type=\"lsa\", top_n=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated Query: {query}\")\n",
    "    print(\"\\nPersonalized Recommendations:\")\n",
    "    for i, (_, row) in enumerate(recommendations.iterrows()):\n",
    "        print(f\"{i+1}. {row['title']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"   Category: {row['category']}\")\n",
    "        print(f\"   Tags: {row['tags']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01441dbe",
   "metadata": {},
   "source": [
    "## 8. Model Export\n",
    "\n",
    "Now that we've built and evaluated our model, let's export it for use in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model package with all the necessary components\n",
    "model_package = {\n",
    "    \"vectorizer\": tfidf_vectorizer,\n",
    "    \"svd\": svd,\n",
    "    \"lsa_matrix\": lsa_matrix,\n",
    "    \"tfidf_matrix\": tfidf_matrix,\n",
    "    \"articles\": articles_df[['article_id', 'title', 'category', 'tags', 'content']].to_dict('records')\n",
    "}\n",
    "\n",
    "# Save the model package\n",
    "with open(RECOMMENDER_MODEL_PATH, 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "print(f\"Model package saved to {RECOMMENDER_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e01f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified version of the articles data for the app\n",
    "articles_data = []\n",
    "for _, article in articles_df.iterrows():\n",
    "    articles_data.append({\n",
    "        \"id\": article['article_id'],\n",
    "        \"title\": article['title'],\n",
    "        \"category\": article['category'],\n",
    "        \"tags\": article['tags'].split(\", \"),\n",
    "        \"summary\": article['content'][:200] + \"...\" if len(article['content']) > 200 else article['content']\n",
    "    })\n",
    "\n",
    "# Save the articles data as JSON for the app\n",
    "with open(RECOMMENDER_MODEL_PATH, 'w') as f:\n",
    "    json.dump({\"articles\": articles_data}, f)\n",
    "print(f\"Articles data saved to {RECOMMENDER_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fb8ac",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we developed a robust article recommendation system for the NutriGenius application. Our system can provide personalized article recommendations based on user demographics, detected food items, and health status.\n",
    "\n",
    "### Summary of achievements:\n",
    "1. Built a TF-IDF vectorization model for article content representation\n",
    "2. Applied LSA for dimensionality reduction and latent semantic understanding\n",
    "3. Developed article-to-article recommendation functionality\n",
    "4. Implemented query-based recommendation for user searches\n",
    "5. Created personalized recommendation logic based on user profiles\n",
    "6. Evaluated and compared different recommendation approaches\n",
    "7. Exported the model for use in the mobile application\n",
    "\n",
    "### Performance insights:\n",
    "- LSA recommendations tend to capture more semantic relationships between articles\n",
    "- TF-IDF recommendations are often more focused on keyword matching\n",
    "- Personalized recommendations successfully prioritize content relevant to user profiles\n",
    "\n",
    "### Next steps:\n",
    "1. Integration with the mobile application\n",
    "2. Collection of user feedback to improve recommendations\n",
    "3. Exploration of hybrid recommendation approaches (content + collaborative filtering)\n",
    "4. Regular updates to the article database "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
